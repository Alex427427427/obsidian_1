**Conductor**: Alexander Li, student ID 30630711, contactable via alii0024@student.monash.edu
**Date**: 2024 August 6
# Risk Assessment
![[risk assessment.png]]
*Figure 1: Risk Assessment for Experiment 3.3*
# Aim
To perform spatial filtering of images with the Fourier optical functions of a lens, and observe how the image is affected. 
# 1 Background Theory
There are multiple ways to derive Fourier optics. In the following I choose the approach most natural to me - integration of wavelets. 
### 1.1 Principles of Fourier Optics
The local mechanism of light propagation can be modelled as spherical waves emerging from infinitesimal point sources everywhere in space, forming wavefronts by superposition. This is Hyugen's principle. Since the development of electromagnetic theory, the physical mechanism of light propagation in materials was finally derived, and indeed found to be consistent with Hyugen's principle. Every location in a medium acts as an infinitesimal local oscillator, polarising and magnetising as previous wavefronts pass through, generating their own spherical wavelets to form the next wavefront. (**Source: 2016, Hecht E, Optics, Chapter 3.5 - Light in Bulk Matter**)

Fourier optics is the mathematically rigorous application of this idea. The resultant wave amplitude at some detection location, is a sum of spherical waves of the form $e^{ikr}/r$ over a spatial distribution of all sources. We shall carry this analysis through to see the Fourier transform emerge. 

Suppose there is a plane on which the amount of light that is let through depends on some spatial **aperture function** $A(x)$. (For example, a hard slit would be 1 for the range of $x$ values where there is no barrier, and 0 everywhere else. 1 indicating that all incident light is let through, 0 indicating no incident light is let through.) 
![[fourier optics expl 1.png]]
*Figure 2: Diffraction Set up*

Then Hyugen's principle states that the wave amplitude $\Psi$ at some location $z$ away from the slit and $x$ off the optical axis would be: 
$$\Psi(z,x)\propto\int_{-\infty}^{\infty}A(x^\prime)\frac{e^{ikr}}{r}dx^\prime$$
Which, when the geometry has been expressed explicitly in terms of $z$ and $x$, becomes:
$$\Psi(z,x)\propto\int_{-\infty}^\infty A(x^\prime)\frac{e^{ik\sqrt{z^2+(x-x^\prime)^2}}}{\sqrt{z^2+(x-x^\prime)^2}}dx^\prime$$
This is the integral that gives the exact wave amplitude. 
### 1.2 Fresnel Approximation
The intermediate field, or **Fresnel zone**, is a region where the **para-axial approximation** is valid. In other words, $z\gg x$, $z\approx r$, and using the binomial approximation, the wave amplitude simplifies to: 
$$\Psi(z,x)\propto\frac{e^{ikz}}{z}\int_{-\infty}^\infty A(x^\prime)e^{ik(x-x^\prime)^2/2z}dx^\prime$$
Which is the **Fresnel integral**. 
### 1.3 Far Field Approximation
We can yet simplify the equation more in the far field, which begins at the Fraunhofer distance $d_F$:
$$\boxed{d_F=2\frac{W^2}{\lambda}}$$
where $W$ is the maximum dimension of the radiating element. (**Source: 1982, Balanis C, Antenna Theory, Chapter 2.2.4 Field Regions**)

Physically, this corresponds to where we are so far that any tiny angular deviation from the optical axis results in a huge displacement $x$, compared to the original total dimension of the aperture function. In other words, $x\gg x^\prime$, which, when the Fresnel integral is evaluated to first order with this in mind, gives: 
$$\boxed{\Psi(z,x)\propto\frac{e^{ikz}}{z}e^{ik_xx/2}\int_{-\infty}^\infty A(x^\prime)e^{-ik_xx^\prime}dx^\prime}$$
This is the **Fraunhofer integral**, and where the form of the Fourier transform emerges. Note that $k_x:=kx/r\approx kx/z$. The terms outside the integral do not affect the intensity along $x$ at a fixed $z$, so we can say that **the intensity pattern $|\Psi|^2$ is proportional to the square of the Fourier transform of the aperture function**. In 2 dimensions: 
$$\boxed{\Psi(z,x,y)\propto\frac{e^{ikz}}{z}e^{i(k_xx+k_yy)/2}\iint_{Aperture} A(x^\prime,y^\prime)e^{-i(k_xx^\prime+k_yy^\prime)}dx^\prime dy^\prime}$$
More properties of this equation for the wave amplitude distribution will be discussed later when appropriate. 
### 1.4 Application to Images and Lenses
A monochromatic image is really just equivalent to some spatial aperture function. As spherical light propagates away from every point on the image into the far field region, the intensity distribution over a wavefront eventually becomes the square of the Fourier transform of the image. 

Now imagine instead of letting light propagate all the way into the far field region, we insert a lens at one focal length away. Since the lens turns points on the focal plane into plane waves on the other side, it's as if we skipped all the propagation straight into the far field region where rays are parallel to one another - we went straight to the Fourier transform. The reverse is true - incoming plane waves (which by now are in the far field relative to the source) get focused to a point on the focal plane of the lens to create an image, which makes the process an inverse Fourier transform. 

By feeding the light from an image through a lens placed at one focal length away, we can directly manipulate the Fourier transform of the image, and use another lens further along to inverse Fourier transform, resulting in the altered image at a focal length away from this second lens. 
# 2 Preparatory Questions
### 2.1 Fraunhofer Pattern
![[prep q1.png]]
*Figure 3: Near and Far Field Diffraction Pattern Ray Diagram*

- The near field transforms into the far field as explained in the previous section, $2D^2/\lambda$
- Both the Fresnel near field pattern and the Fraunhofer far field pattern are created by interference between various angular components of the wave. 
- But in the near field case, the interference never completely destroys the intensity at a particular spot, because the angular variation of rays that combine at a particular spot is still high enough for light from certain components to hit places that would otherwise be destructively interfered by other components. Geometrically we see this as rays of different angles hitting the same spot. 
- By the time we reach the far field, the angular distribution have completely resolved, leaving the rays to neatly march onward without further change in the pattern. By this region, the rays that hit a particular spot are basically parallel, and total destructive interference can occur. 
### 2.2 Image at Fourier Plane
![[prep q2.png]]
*Figure 4: Fourier Planes of a Lens*

- We would see a radial distribution that looks like the square of $sinc(\rho)$, for that is the Fourier transform of $rect(\rho)$, which is what a circular aperture is. The angular distribution should be uniform, for the aperture was rotationally symmetric. 
- The mathematical formalism that relates the aperture and the screen is the Fourier transform. 
### 2.3 Diffraction Maxima
![[prep q3.png]]
*Figure 5: Diffraction Maxima*

- The $m$th maximum on the screen must correspond to the path difference of rays from each element to be $m$ full cycles from one another, so that they maximally constructively interfere. 
- In the limit of far field, the rays are parallel to one another. Thus there is a single expression of path difference that is valid for all those parallel rays: 
$$d\sin(\theta)=m\lambda$$
- In the small angle approximation, $\sin(\theta)\approx\theta$. Rearranging for $\theta$ gives:
$$\boxed{\theta=\frac{m\lambda}{d}}$$
# 3 Log of the Experiment
### 3.1 Initial Set Up
- On top of a rail, I set up a green laser at one end. And, in order from one end of the rail to the other: a grating which produces a grid image, a lens at roughly one focal length away from the grating, a filter at roughly one focal length away from the lens, and a mirror at the other end. The focal length is taken to be the label on the lens for now - 250mm. The mirror helps increase the total propagation of the laser by two fold, because the camera (hidden behind a scattering screen) is placed next to the laser, facing the mirror - this allows us to perform inverse Fourier transform due to the propagation into the far field. 
![[setup 1.png]]
*Figure 6: Initial Set up*

![[setup 2.png|200]]
*Figure 7: Mirror in Set up*
### 3.2 Camera Configuration
- Realising I didn't need the filter yet, I took it off the rail. 
- I connected the camera to my laptop via USB, and started ThorCAM to view the image. I confirmed the image matches what i see on the scattering screen. 
![[first image.png|200]]
*Figure 8: First Image*

- I noticed the image shown on software is rotated by 90 degrees compared to the physical set up upon wiggling the camera. There is no software setting for rotation and no mechanical equipment to change the orientation of the camera about the roll axis, so I keep this in mind for the rest of the experiment. 
- I adjusted the exposure for the camera until the image was not too bright. 
- I calibrated the scale of the camera by aiming it at a white background (the ceiling), placing a short ruler on top of it, and capturing an image. 
![[calibration image.png|200]]
*Figure 9: Calibration Image*

- In an image processing software ImageJ, I found that a straight line connecting markers 5cm apart corresponds to 1109 pixels - which gives the scale:
$$scale=\frac{1109\ pixels}{50\ mm}\approx 22.18\ pixels/mm$$
- Noticing that my measurement on the screen can be off by around 5 pixels, I found that the uncertainty in the scale is 0.1 pixels/mm. Thus the scale is:
$$\boxed{scale=22.2\pm0.1\ pixels/mm}$$
### 3.3 Determining the Focal Length of the Lens
- I then determined the focal length of the lens in practice. 
- I fixed the grating (image) at the 25.00cm marking on the rail, the far mirror at 200cm, and the camera's scattering screen at 1cm. I moved the lens around until the image on the scattering screen became sharp. The lens had to sit at 52.20cm.  
- This meant the object distance was 27.20cm. 
- I just noticed that the actual locations of the grating and the lens do not line up with the markings on the rail. Rotating them by 180 degrees in their mount fixed this issue. 
- With the correction, the lens had to be moved to 52.00cm. Which means the object distance is 27.00cm. 
- Since the distance from the lens to the mirror is 148.00cm, and the distance from the mirror to the camera scattering screen is 199cm, the image distance is 347cm. 
- With the thin lens formula $1/f=1/d_o+1/d_i$, I rearranged for $f$, and found that:
$$f_1=\frac{d_id_o}{d_i+d_o}\approx 25.05cm$$
- I put the calculations into a python file. Assuming the alignment of the camera next to the rail has an uncertainty of 1cm, and that the rail markings have 0.01cm uncertainty, propagating the uncertainty to the focal length via quadrature gives an uncertainty of $0.14cm$. Keeping 1 significant figure of the uncertainty, I arrive at: 
$$\boxed{f_1=25.1\pm0.1cm}$$
- I repeated the same procedure for lens 2, and found its focal length to be: 
$$\boxed{f_2 = 15.2\pm0.1cm}$$
### 3.4 Measuring Bar Period of the Grid
- I returned the set up to lens 1, chose the medium sized grids at the first aperture, and adjusted the position of lens 1 until the image of the grid was clear on the scattering screen of the camera. Since the lens was at 51.88cm, the object distance must be $51.88 - 25.00 = 26.88cm$, and the image distance is $199 + (200-51.88) = 347.12cm$. This corresponds to a thin lens magnification of: 
$$M=\frac{d_i}{d_o}\approx12.91$$
- Propagating the uncertainties (taken to be the same as argued above, I obtained:
$$\boxed{M=12.91\pm0.04}$$
- So I proceeded to save the image and measure the grid period in pixels. To avoid the uncertainty in the gridline thickness, I took measurement from a corner to another corner. To further reduce uncertainty, i took measurement over 8 grids. It was 586 pixels. 
- I measured 8 grids at another location on the image, and got 584 pixels. Taking the average, I got 585 pixels, with an uncertainty of 1 pixel. 
- Using the scale and propagating uncertainties, I calculated the grid width $w_i$ of the image to be: 
$$w_i=\frac{585/8}{scale}=3.29\pm0.02\ mm$$
- Using the magnification and propagating uncertainties, I calculated the actual grid width $w_o$ of the object to be:
$$\boxed{w_o=\frac{w_i}{M}=0.255\pm0.001\ mm}$$
### 3.5 Estimating Near Field - Far Field Transition
- I removed the lens, and held a white paper to receive the light passing through the grid object. I moved the white paper further and further away from the grid, and observed the patterns. 
- At close to the grid, I saw the Fresnel diffraction pattern - where individual lobes were present, but overlap and are not fully resolved. 
![[FT 1.png|200]]
*Figure 10 - Fresnel Diffraction Pattern*

- Moving beyond the 50cm marking on the rails, I saw the lobes become distinct, and proper dark bands to appear, separating the lobes. This is the Fraunhofer diffraction pattern. 
![[FT 2.png|200]]
*Figure 11 - Fraunhofer Diffraction Pattern*

- However, as I moved even further, I realised that *each of the individual lobes* contained many smaller sub-lobes, which were still overlapping to form the larger lobes previously observed. 
![[FT3.png|200]]
*Figure 12: Sub lobes first appear*

- As I traced the image all the way to the camera, I saw that those sub-lobes were beginning to separate and become independent, but they still were infringing on one another. 
![[FT4.png|200]]
*Figure 13: Sub lobes at the camera*

![[sub lobes.png|200]]
*Figure 14: Close up of sub lobes*

- I compared these results to the theoretical prediction - that the far field begins at $2W^2/\lambda$. Where $W$ is the maximum dimension of the radiating element, and $\lambda = 531.7\pm0.5\ nm$ which was the wavelength of the laser. Taking $W=9w_o$ because the total diameter of the object was 9 grids, I found the far field should begin at $$z=2\frac{W^2}{\lambda}\approx20\ m$$
- This obvious did not match the original location of 50cm where the lobes began to resolve, which was a distance of 25cm away from the object. 
- However, if we take the width $W$ to be a *single grid*, which was a dominant spatially periodic feature of the object, then we get: 
$$z=2\frac{w_o^2}{\lambda}\approx 24\ cm$$
- So the full picture is: as we reach beyond 24cm, one dominant spatial periodic mode becomes resolved in the pattern as large lobes - the one associated with the grids themselves. As we move further, the total dimension of the grid object becomes resolved in the pattern as small sub-lobes for each large lobe. If we were able to capture the whole image at beyond 20m, we should have been able to see these sub-lobes to be completely separated, and we will have obtained the complete Fourier transform of the original object. 
- To confirm the above, I emulated a full far-field propagation by placing lens_1 at 1 focal length away from the grid object, and placing the camera at the Fourier plane on the other side of the lens. I placed a ruler at the original location of the camera so I could return the camera to it later. 
- By adjusting the exposure, I saw that the pattern was indeed one where every large lobe had a sea of smaller cleanly resolved lobes accompanying each. 
![[FT grid low exp.png|200]]
*Figure 15: Fourier Transform, low exposure*

![[FT grid high exp.png|200]]
*Figure 16: Fourier Transform, high exposure*
### 3.6 Measuring the Diffraction Pattern
- With the camera still on the rail, I removed the lens, and measured the diffraction pattern in pixels at distances with 20cm increments. This is for the medium sized grids. 

**Table 1: Distance vs Lobe width of the Diffraction Pattern** 

| camera distance from object (cm) | Lobe width (pixels) |
| -------------------------------- | ------------------- |
| $60\pm1$                         | $162\pm2$           |
| $80\pm1$                         | $223\pm5$           |
| $100\pm1$                        | $269\pm8$           |
| $120\pm1$                        | $331\pm7$           |
- I will perform a fit and find out the width of the original grid in later sections. 
### 3.7 Spatial Filtering with Slit
- I placed the camera back at the ruler marking. I placed lens 1 at a location such that the image appears sharp at the camera screen. So lens 1 was at the 51.88cm marking on the rails. 
- I placed a slit filter at about one focal length away from the lens, and adjusted the mount until the slit is centered at the center of the Fourier pattern. 
- For various grid sizes, I recorded the images of various amounts of filtering. 

**Table 2: Various vertical filtering of various grid sizes**

| grid size | large vertical slit         | medium vertical slit        | miniscule vertical slit     |
| --------- | --------------------------- | --------------------------- | --------------------------- |
| fine      | ![[fine vert 1.png\|100]]   | ![[fine vert 2.png\|100]]   | ![[fine vert 3.png\|100]]   |
| medium    | ![[med vert 1.png\|100]]    | ![[med vert 2.png\|100]]    | ![[med vert 3.png\|100]]    |
| coarse    | ![[coarse vert 1.png\|100]] | ![[coarse vert 2.png\|100]] | ![[coarse vert 3.png\|100]] |
- **Note that these images should be rotated by 90 degrees.** 
- So we see that the **horizontal features** get more and more smeared out as we reduce the width of the vertical slit. The vertical features are unaffected. This is because filtering is only done to the horizontal spatial frequencies. 
- Another observation: smaller grid sizes correspond to sparser Fourier transforms, which makes sense because small grid size means higher spatial frequency. As a result, it is easier to pinch the fourier transform of smaller grid sizes, because the lobes are more sparse and can be discretely filtered. 
- I repeated the above procedure for horizontal slits. Since the minimum height of the horizontal slit was still to high, I had to adjust every element in the apparatus to be slightly higher. 

**Table 3: Various horizontal filtering of various grid sizes**

| grid size | large horizontal slit       | medium horizontal slit      | miniscule horizontal slit   |
| --------- | --------------------------- | --------------------------- | --------------------------- |
| fine      | ![[fine hori 1.png\|100]]   | ![[fine hori 2.png\|100]]   | ![[fine hori 3.png\|100]]   |
| medium    | ![[med hori 1.png\|100]]    | ![[med hori 2.png\|100]]    | ![[med hori 3.png\|100]]    |
| coarse    | ![[coarse hori 1.png\|100]] | ![[coarse hori 2.png\|100]] | ![[coarse hori 3.png\|100]] |
- Again, **the actual images formed are rotated by 90 degrees**. 
- So we see that the **vertical features** get smeared out, due to the killing of higher spatial frequencies in the vertical direction by the horizontal slit. The horizontal features are unaffected.
### 3.8 Iris Aperture 
- I replaced the slit filter with the iris filter, aligning the center of the iris with the center of the beam. 
- I recorded the medium grids at various closings of the iris aperture. 

**Table 4: Gradually Closing the Iris Aperture**

| ![[iris 1.png\|100]] | ![[iris 2.png\|100]] | ![[iris 3.png\|100]] | ![[iris 4.png\|100]] | ![[iris 5.png\|100]] |
| -------------------- | -------------------- | -------------------- | -------------------- | -------------------- |
- I see that the image becomes more and more blurred circularly, as if a gaussian blurring filter is being applied to the image. Of course, we know that the actual filter is a sharp circle, but at a certain limit, the circle allows exactly one gaussian beam to pass through. 
- I then probed the effect of linearly increasing the aperture size, allowing integer number of spots on the Fourier plane to pass through. 

**Table 5: Linear Increase of Iris Aperture**

| lobes | 1                         | 2                         | 3                         | 4                         |
| ----- | ------------------------- | ------------------------- | ------------------------- | ------------------------- |
| image | ![[iris lobe 1.png\|100]] | ![[iris lobe 2.png\|100]] | ![[iris lobe 3.png\|100]] | ![[iris lobe 4.png\|100]] |
| lobes | 5                         | 6                         | 7                         | fully open                |
| image | ![[iris lobe 5.png\|100]] | ![[iris lobe 6.png\|100]] | ![[iris lobe 7.png\|100]] | ![[iris lobe 8.png\|100]] |
- The gridlines become clearer and clearer, and more square. By order 5, even miniscule interference artefacts are not really noticeable. 
- We see that starting from order 5, the image has no noticeable difference to when the iris is fully open. 
### 3.9 Central Spot
- I replaced the iris aperture with a glass slide containing a dark spot, blocking out the center of the Fourier-transformed beam. 
![[central spot removed.png|200]]
*Figure 17: Image after removing central spot in the Fourier plane*

- I see that the flat regions are suppressed and the edges become highlighted. From the Fourier point of view, we have just removed the component that corresponds to the average intensity of the image, leaving behind only the components that have non zero spatial frequency and thus have some spatial change - it makes me wonder, whether this could work as an edge detector in computer vision pipelines. 
### 3.10 Wrap Up
- With all aspects of the experiment explored, I shut down the laser, camera, and returned the various optical elements to their stand. 
# 4 Further Analysis
### 4.1 Finding the Grid Size from the Diffraction Pattern
This section is the continuation of 3.6 Measuring the Diffraction Pattern. I had obtained the diffraction widths in **Table 1**. I write some python code to perform a linear fit and calculate the size of the grid. 
![[linear fit 1.png]]
*Figure 18: Linear fit of lobe width against image distance from aperture*

- Using the equation derived in **section 2.3**:
$$\theta\approx\frac{x}{z}=\frac{m\lambda}{d}\implies x=\frac{m\lambda}{d}z$$
- Thus the lobe width - the distance between subsequent maxima, is:
$$w=\frac{\lambda}{d}z$$
- The slope of the linear fit of $w$ against $z$ is equal to $\lambda/d$, so the grid size $d$ must be:
$$d=\frac{\lambda}{slope}=0.042\pm0.002\ mm$$
Where the uncertainty was propagated by quadrature. This is quite far from the result obtained from the thin lens magnification, of $d=0.255\pm0.001\ mm$. The results do not overlap even with 3 standard uncertainties. So something is wrong. 

- I returned to the images and decided to measure the sub-lobe widths, rather than the large lobe widths. 
![[large lobe meas.png|200]]
*Figure 19: Previously Measuring Large Lobe Widths*

![[small lobe meas.png|200]]
*Figure 20: Measuring Sub-lobe Widths*

- Of course, to reduce the uncertainty in the measurements, I took 8 sub-lobes of the central major lobe, and divided by 8. The results are in **Table 6**.

**Table 6: New Lobe Width Measurements**

| camera distance from object (cm) | Sub-Lobe width (pixels) |
| -------------------------------- | ----------------------- |
| $60\pm1$                         | $29\pm1$                |
| $80\pm1$                         | $37\pm1$                |
| $100\pm1$                        | $46\pm1$                |
| $120\pm1$                        | $50\pm1$                |

- With this, I repeated the linear fit, shown in **Figure 21**:
![[linear fit small.png]]
*Figure 21: Linear Fit of the Sub-lobe Widths against Image Distance from Aperture*

- Using the slope of this linear fit, I found the corresponding d: 
$$\boxed{d=0.33\pm0.07\ mm}$$
- I visualise the predictions of the grid width (bar period) obtained via the thin lens magnification and via the diffraction pattern, by plotting them as gaussian distributions (unnormalised, because the normalised graph would be too tall). The result is in **Figure 22**. 
![[prediction comparison.png]]
*Figure 22: Comparison of Grid Size Predictions*

- This time, the predictions overlap. The prediction from the thin lens equation is vastly more precise than the prediction from the diffraction pattern. 
### 4.2 Computationally Calculating the Fourier Transforms
- One thing I'm interested is how closely would the actual Fourier transform match with the diffraction pattern. I took various images collected throughout the experiment and calculated the Fast Fourier Transform using the Numpy library. The results are summarised in **Table 7**. 
- In order to perform Fourier Transform, the image must be turned into grayscale. 
- I also had to perform a logarithm transform, otherwise the FFT image are just a single dot at the center. (But the images *after* the log transform were what I saw in the physical experiment. Do human eyes perceive intensity on a logarithm scale?)
![[Pasted image 20240809030855.png]]
*Figure 23: FFT Without Log Transform*

![[Pasted image 20240809030130.png|330]]
*Figure 24: FFT With Log Transform*


**Table 7: Fourier Transform of Images**

|                     | Original Image              | Computer FFT Result                       |
| ------------------- | --------------------------- | ----------------------------------------- |
| Unfiltered          | ![[med gray.png\|200]]      | ![[Pasted image 20240809030130.png\|200]] |
| Horizontal Smearing | ![[med vert gray.png\|200]] | ![[Pasted image 20240809030255.png\|200]] |
| Vertical Smearing   | ![[med hori gray.png\|200]] | ![[Pasted image 20240809030336.png\|200]] |
| Iris, order 2       | ![[2 lobe gray.png\|200]]   | ![[Pasted image 20240809030414.png\|200]] |
| Iris, order 1       | ![[1 lobe gray.png\|200]]   | ![[Pasted image 20240809030739.png\|200]] |
- I see that the computer FFT reproduces the diffraction results almost perfectly - the FFTs correspond to the various filtered Fourier plane images. 
# 5 Further Discussion
This section will only cover what has not already been extensively discussed throughout the experimental logs. There are two main left over issues at hand, plus some further extensions. 
### 5.1 What Do the Major Lobes and Sub-Lobes Really Correspond To? 

### 5.2 Why Thin Lens was More Precise than Diffraction Pattern in Estimating Grid Size

### 5.3 Laplace Transform Experiment

### 5.4 Possible Application 1: Hardware FT Photonic Module
FFT may be the most important algorithm of all time (**Source: Strang, Gilbert (May–June 1994). "Wavelets". American Scientist. 82 (3): 250–255.**). Wouldn't it be nice to have a hardware module that calculates the Fourier Transform of some signal at the speed of light propagation? But miniaturisation may be limited by the wavelength of light that can be realistically used. And the fast speed of light propagation may be cancelled out by the time needed to convert whatever digital signal that we want the FT of, into an optical pattern, and vice versa. 

Nevertheless, there has been ideas proposed to create such a module. (**Source: Macfaden, A.J., Gordon, G.S.D. & Wilkinson, T.D. An optical Fourier transform coprocessor with direct phase determination. Sci Rep 7, 13667 (2017). https://doi.org/10.1038/s41598-017-13733-1.**)
### 5.5 Possible Application 2: Hardware Edge Detector Module
In **Section 3.9**, I saw that removing the central spot gets rid of the component of the image that has no spatial frequency, leaving behind the components that change. Removing a larger area around the center would result in the lower spatial frequencies to be filtered out. 

Sharp edges are among the highest varying frequencies, so I wonder if we can create a hardware module that uses lenses and opaque spots to filter for the edges in an image. 
### 5.6 Human Perception is Logarithmic
In **Section 4.2**, I performed FFT on the images. In order to make the result match what I saw in the experiment, I had to perform a log transform on the magnitude of the FFT output. I did some research, and found that this is indeed the case for human perception - we perceive intensity (brightness, loudness) on a logarithmic scale. This is known as **Fechner's Law** (**Source: Wikipedia, Weber-Fechner Law**).
# Conclusion

# References
Located throughout the logbook. 